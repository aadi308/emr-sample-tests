Deploying the Jobs on emr on eks with log group
The Script given below will execute the forcast job.

export AWS_PROFILE=emr-nonprod-from-root-fpg-acc
export AWS_REGION=us-west-2
export VIRTUAL_CLUSTER_ID=$(aws emr-containers list-virtual-clusters --query "virtualClusters[?state=='RUNNING'].id" --output text)
export EMR_ROLE_ARN=$(aws iam get-role --role-name emrcontainers-jobexecutionrole-hotel-nonprod --query Role.Arn --output text)

aws emr-containers start-job-run \
  --virtual-cluster-id=$VIRTUAL_CLUSTER_ID \
  --name=forcast-job \
  --execution-role-arn=$EMR_ROLE_ARN \
  --release-label=emr-6.6.0-latest \
  --job-driver='{
    "sparkSubmitJobDriver": {
      "entryPoint":"local:///fpg_spark_hospitality/call_forecast_proc.py",
      "sparkSubmitParameters": " --jars   local:///fpg_spark_hospitality/jars/mysql-connector-java-8.0.29.jar  --conf spark.dynamicAllocation.shuffleTracking.enabled=true --conf spark.executor.instances=1 --deploy-mode cluster --conf spark.executor.memory=1G --conf spark.executor.cores=1 --conf spark.driver.cores=1 --conf spark.kubernetes.container.image=447722449887.dkr.ecr.us-west-2.amazonaws.com/custom-emr:latest "
    }
  }'\
  --configuration-overrides='{

        "applicationConfiguration": [
      {
        "classification": "spark-defaults",
        "properties": {
          "spark.sql.parquet.datetimeRebaseModeInWrite":"CORRECTED",
      "spark.dynamicAllocation.enabled":"true",
          "spark.dynamicAllocation.minExecutors":"1",
          "spark.dynamicAllocation.maxExecutors":"25",
      "spark.dynamicAllocation.initialExecutors":"1",
          "spark.yarn.driver.CONFIG_FILE":"k8s_hotel_forecast.json",
          "spark.yarn.driver.RUN_TYPE":"complete"
         }
      }
    ],
    
     "monitoringConfiguration": {
      "cloudWatchMonitoringConfiguration": {
        "logGroupName": "/emr-on-eks/jobs",
        "logStreamNamePrefix": "forcast-job"
      },
      "s3MonitoringConfiguration": {
        "logUri": "s3://sparkjob-logs/bigjob-logs/"
      }
    }
  }'


once the job is submitted, we can view the current status and state of the jobs on the emr-cluster lists.

Deploying the Jobs on emr on eks with log group and pod tempalates
Pod Priorities
On the basis and kind of the jobs we can prioritise the job run in eks. we use following template to prioritise the job runs.

apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: critical-priority
value: 400
globalDefault: true
description: "critical-priority Pods"

Creating the pod Templates
For a generic pod templates is used for division of the instance capacity types for spark driver and executor. The driver pods are scheduled to the ON-Demand Capcity types where as the executor pods are scheduled to the SPOT capacity types.

spark_driver-pod-template.yml
apiVersion: v1
kind: Pod
spec:
  priorityClassName: "critical-priority"
  nodeSelector:
    eks.amazonaws.com/capacityType: ON_DEMAND
  containers:
  - name: spark-kubernetes-driver 


spark_executor_pod-template.yaml
apiVersion: v1
kind: Pod
spec:
  # Reusing the critical priority class
  priorityClassName: "critical-priority"

  nodeSelector:
    eks.amazonaws.com/capacityType: SPOT
  containers:
  - name: spark-kubernetes-executor

The pods templates file are provided from the s3 bucket to the emr job run scripts.

export AWS_PROFILE=emr-nonprod-from-root-fpg-acc
export AWS_REGION=us-west-2
export VIRTUAL_CLUSTER_ID=$(aws emr-containers list-virtual-clusters --query "virtualClusters[?state=='RUNNING'].id" --output text)
export EMR_ROLE_ARN=$(aws iam get-role --role-name  emrcontainers-jobexecutionrole-hotel-nonprod --query Role.Arn --output text)

aws emr-containers start-job-run \
  --virtual-cluster-id=$VIRTUAL_CLUSTER_ID \
  --name=checktransform \
  --execution-role-arn=$EMR_ROLE_ARN \
  --release-label=emr-6.6.0-latest \
  --job-driver='{
    "sparkSubmitJobDriver": {
      "entryPoint":"local:///fpg_spark_hospitality/check_and_transform.py",
      "sparkSubmitParameters": " --jars   local:///fpg_spark_hospitality/jars/mysql-connector-java-8.0.29.jar   --conf spark.executor.instances=1 --deploy-mode cluster --conf spark.executor.memory=10G --conf spark.executor.cores=3 --conf spark.driver.memory=10G --conf spark.kubernetes.container.image=447722449887.dkr.ecr.us-west-2.amazonaws.com/custom-emr:latest  --conf spark.kubernetes.executor.podTemplateFile=s3://forcast-emr-buckets/podtemplates/spark_executor_pod_template.yaml  --conf spark.kubernetes.driver.podTemplateFile=s3://forcast-emr-buckets/podtemplates/spark_driver_pod_template.yaml "
    }
  }'\
  --configuration-overrides='{

        "applicationConfiguration": [
      {
        "classification": "spark-defaults",
        "properties": {
          "spark.sql.parquet.datetimeRebaseModeInWrite":"CORRECTED",
      "spark.dynamicAllocation.enabled":"true",
      "spark.kubernetes.executor.deleteOnTermination": "true",
      "spark.dynamicAllocation.shuffleTracking.enabled":"true",
          "spark.dynamicAllocation.minExecutors":"1",
          "spark.dynamicAllocation.maxExecutors":"25",
      "spark.dynamicAllocation.initialExecutors":"1",
          "spark.yarn.driver.CONFIG_FILE":"dev_hotel.json",
          "spark.yarn.driver.RUN_TYPE":"complete"
         }
      }
    ],
     "monitoringConfiguration": {
      "cloudWatchMonitoringConfiguration": {
        "logGroupName": "/emr-on-eks/jobs", 
        "logStreamNamePrefix": "check-and-transform"
      }, 
      "s3MonitoringConfiguration": {
        "logUri": "s3://sparkjob-logs/bigjob-logs/"
      }
    }
    
  }'


Building the custom images for the EMR on EKs job runs
The image is built usin the image provided by the aws itself. we build the custom image with the required dependencies and uploaded to the ECR from where the eks will pull the image for the josb runs.

Dockerfile

FROM 895885662937.dkr.ecr.us-west-2.amazonaws.com/spark/emr-6.6.0:latest
USER root
RUN pip3 install  boto3 pandas numpy pyspark mysql-connector-python
#<copyin the repo files for local availability>
COPY .   /  
RUN chmod -R +r  /usr/lib/spark/jars
USER hadoop:hadoop
